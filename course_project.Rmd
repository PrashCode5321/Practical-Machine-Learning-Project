---
title: "Practical Machine Learning Project"
author: "Prashant"
date: "9/27/2022"
output:
  html_document:
    df_print: paged
  md_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Background  
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har>.  

### Data  
  
The training data for this project are available here:  

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>  
  
The test data are available here:  

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>  
  
### Loading libraries

```{r warning=FALSE}
library(caret)
library(rattle)
library(knitr)
library(randomForest)

```

### Loading datasets  
Loading the training and the testing data into `df_train` and `df_test` with the setting that any instances of `NA` and `""` will be considered as `NA`.

```{r cache = TRUE}
df_train <- read.csv('pml-training.csv', na.strings = c("NA", ""))
df_test <- read.csv('pml-testing.csv', na.strings = c("NA", ""))
```
Checking the dimensions of the training and testing data:  
```{r}
dim(df_train)
```

```{r}
dim(df_test)
```

Checking out the names of the columns in the dataset:  
```{r}
names(df_train)
```
The name of the target column is `classe`.  

### Data Splitting  
Splitting the `df_train` into training and validation sets for **cross validation**:  
```{r cache = TRUE}
train_idx <- createDataPartition(df_train$classe, p = 0.7, list = FALSE)
X_val <- df_train[-train_idx, ]
df_train <- df_train[train_idx, ]
```

Checking out the dimensions of these datasets:  
```{r}
dim(df_train)
```

```{r}
dim(X_val)
```
  
### Data Preprocessing  

Checking for the instances of missing values:  
```{r}
colSums(is.na(df_train))
```

Many columns tend to have missing values.  
Checking what percentage of data is missing in these columns:  
```{r}
colMeans(is.na(df_train))
```

Nearly 98% data is missing in these columns!  
There is no point in applying KNN Imputation on them.  
Hence it would be best to drop these columns:  
```{r cache = TRUE}
X_train <- df_train[ , colSums(is.na(df_train)) == 0]

#removing those columns which were removed in training set
X_val <- X_val[ , colSums(is.na(df_train)) == 0]
```

Saving the character class column names separately for future references:  
```{r cache = TRUE}
cat_cols <- names(df_train[sapply(df_train, is.character)])
```
The reason for creating `cat_cols` is that if the models perform poorly, an alternative approach can be explored where I drop the categorical columns and then train the models.  
Since the models of choice are tree based, there is no need for **Data Scaling** since these models do not require feature scaling.  
  
  
### Modeling  
#### Decision Tree
Training a decision tree on `X_train` and evaluating its on performance of predicting `X_val`:  
```{r}
set.seed(32343)
tree_model <- train(as.factor(classe) ~., data = X_train, method = 'rpart')
tree_predictions <- predict(tree_model, newdata = X_val)
confusionMatrix(tree_predictions, as.factor(X_val$classe))
```

Plotting the decision tree:  
```{r cache = TRUE}
fancyRpartPlot(tree_model$finalModel)
savePlotToFile(file.name = 'tree.png', dev.num=dev.cur())
```
  
The accuracy of the decision tree is quite bad.  
Let's switch to another model in hopes of producing better results.  
  
  
#### Random Forest  
While working on the project, I noticed that the conventional random forest model was taking too long to train. So I used `randomForest()` function for training.  
The documentation for this model : <https://cran.r-project.org/web/packages/randomForest/randomForest.pdf>  
  
Training a random forest on `X_train` and evaluating its on performance of predicting `X_val`:  
```{r cache= TRUE}
set.seed(32343)
#Discarded method: 
#rf_model <- train(as.factor(classe) ~., data = X_train, method = 'rf', importance =TRUE, ntree = 10)

#Replacement:
rf_model <- randomForest(as.factor(classe) ~ .,data=X_train, importance =TRUE, ntree = 10)
```  

Testing model's performance on validation data:  
```{r}
rf_predictions <- predict(rf_model, X_val)
confusionMatrix(rf_predictions, as.factor(X_val$classe))
```
Since the accuracy achieved is good enough, there is no need to try other models like Gradient Boosting.  
  
  
### Conclusions  
Random forest classifier's accuracy is 99.9%!  
This model outweighs decision tree.  
Hence, Random Forest is chosen.  
  
  
  
#### Expected Out Of Sample Error  
Expected out of sample error, i.e. the error rate on new data for the chosen model is 0.01%!  
The random forest model is predicting extremely well on the validation set.  
Since the generalization error of the model is so less, we can be confident about the model's predictions made for the `df_test`.  
    
  
  
#### Submission set Prediction  
```{r}
#pre-processing of the test set : removing those columns which were removed in training set.
X_test <- df_test[ , colSums(is.na(df_train)) == 0]

#predictions 
test_predictions <- predict(rf_model, X_test)
print(test_predictions)
```
  
Thank you for going through this project.  
